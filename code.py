# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cnv4xQ6sqc-dGJggj7sG6FKjGU8wmu0R
"""

import pandas as pd

import numpy as np
from numpy import concatenate
import matplotlib.pyplot as plt
import datetime
from sklearn.model_selection import train_test_split, TimeSeriesSplit
import xgboost as xgb
from sklearn.metrics import mean_absolute_error
from math import sqrt

train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")

"""# Data exploration"""

train.head()

train.head()

test.head()

test.tail()

train.describe()

train.dtypes

print(train.shape,test.shape)

"""# Accuracy measure for forecasting

SMAPE , or symmetrical mean absolute percentage error, is one calculation that you can use to check the accuracy of your forecasting methods. Knowing what the SMAPE formula is and the benefits of using it can help you make better financial decisions.
"""

def smape(A, F):
    return 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))

"""# Seasonality"""

plt.subplot(3, 1, 1)
plt.plot(train.sales[:365*4])
plt.subplot(3, 1, 2)
plt.plot(train.sales[:365])

plt.subplot(3, 1, 3)
plt.plot(train.sales[:31])
plt.show()

rolling_mean = train.sales.rolling(window=7).mean()
print(rolling_mean.shape)
print(rolling_mean[:31])

rolling_mean_week = train.sales.rolling(window=7).mean()

plt.subplot(3, 1, 1)
plt.plot(rolling_mean_week[:365*4])

plt.subplot(3, 1, 2)
plt.plot(rolling_mean_week[:365])

plt.subplot(3, 1, 3)
plt.plot(rolling_mean_week[:31])

plt.show()

rolling_mean_month = train.sales.rolling(window=30).mean()

plt.subplot(2, 1, 1)
plt.plot(rolling_mean_month[:365*4])

plt.subplot(2, 1, 2)
plt.plot(rolling_mean_month[:365])

plt.show()

"""# Data pre-processing"""

data_combine = pd.concat([train,test])
print("size of data_combine",data_combine.shape)

data_combine['date'] = pd.to_datetime(data_combine['date'],infer_datetime_format=True)

data_combine['day'] = data_combine['date'].dt.dayofweek
data_combine['month'] = data_combine['date'].dt.month
data_combine['year'] = data_combine['date'].dt.year
data_combine['week_of_year']  = data_combine.date.dt.weekofyear

data_combine['date_order'] = (data_combine['date'] - datetime.datetime(2013, 1, 1)).dt.days

data_combine.head(10)

data_combine['sale_moving_average_7days']=data_combine.groupby(["item","store"])['sales'].transform(lambda x: x.rolling(window=7,min_periods=1).mean())

data_combine.head(10)

"""We create 60 day before data using the shift function to introduce new features into the data"""

data_combine['sale_moving_average_7days_shifted-60']=data_combine.groupby(["item","store"])['sale_moving_average_7days'].transform(lambda x:x.shift(60))
data_combine['sales_shifted-60'] = data_combine.groupby(["item","store"])['sales'].transform(lambda x:x.shift(60))

data_combine[90:100]

"""We can do other feature engineering, such as including other shifts like:

data_combine['store_item_shifted-10'] = data_combine.groupby(["item","store"])['sales'].transform(lambda x:x.shift(10))
"""

col = [i for i in data_combine.columns if i not in ['date','id','sale_moving_average_7days']]

print("Old train shape: ", train.shape)
train_new = data_combine.loc[~data_combine.sales.isna()]
print("New train shape: ",train_new.shape)
print('---')
print("Old test shape: ", test.shape)
test_new = data_combine.loc[data_combine.sales.isna()]
print("New test shape: ",test_new.shape)

train_new = (train_new[col]).dropna()
print(train_new.shape)

"""# Model building and training"""

y_target = train_new.sales
col = [i for i in data_combine.columns if i not in ['date','id','sales','sale_moving_average_7days']]

X_train, X_test, y_train, y_test = train_test_split(train_new[col], train_new.sales, test_size=0.15, random_state=42)

model_sets=[]
for max_depth in range(3,17,2):
  xgb_model = xgb.XGBRegressor(max_depth=max_depth ,min_child_weight=1)
  xgb_model.fit(X_train,y_train.values,eval_metric=smape)
  model_sets.append(xgb_model)

  y_train_pred_xgb=xgb_model.predict(X_train)
  y_test_pred_xgb=xgb_model.predict(X_test)
  print('smape error: max_depth=', max_depth ,',train:' , smape(y_train.values,y_train_pred_xgb),'test:',smape(y_test.values,y_test_pred_xgb))
  print('MSE train:' , mean_absolute_error(np.log1p(y_train),np.log1p(y_train_pred_xgb)),'test:',mean_absolute_error(np.log1p(y_test),np.log1p(y_test_pred_xgb)))

"""# Model Testing

Choosing the best model based on the lowest SMAPE results
"""

model_sets[3] # Selecting the best model
y_target = train_new.sales
model_sets[3].fit(train_new[col], y_target, eval_metric=smape) # Fitting the selected model

y_train_pred_xgb = model_sets[3].predict(X_train)
y_test_pred_xgb = model_sets[3].predict(X_test)
print('smape error: ','train:' , smape(y_train.values,y_train_pred_xgb),'test:',smape(y_test.values,y_test_pred_xgb))
print('MSE train:' , mean_absolute_error(np.log1p(y_train),np.log1p(y_train_pred_xgb)),'test:',mean_absolute_error(np.log1p(y_test),np.log1p(y_test_pred_xgb)))

forecast = np.rint(model_sets[3].predict(test_new[col]))

result = pd.DataFrame(list(zip(test_new['id'], forecast)), columns =['id', 'sales'])

result.id = result.id.astype(int)
result.sales = result.sales.astype(int)

# Sample of forecasting
result.head(50)

"""# LSTM (basis for comparison vs XGBoost)"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from matplotlib import pyplot
tf.random.set_seed(42)

data_combine_lstm = pd.concat([train,test])
print("size of data_combine",data_combine_lstm.shape)

data_combine_lstm['date'] = pd.to_datetime(data_combine_lstm['date'],infer_datetime_format=True)

data_combine_lstm['day'] = data_combine_lstm['date'].dt.dayofweek
data_combine_lstm['month'] = data_combine_lstm['date'].dt.month
data_combine_lstm['year'] = data_combine_lstm['date'].dt.year
data_combine_lstm['week_of_year']  = data_combine_lstm.date.dt.weekofyear

data_combine_lstm['date_order'] = (data_combine_lstm['date'] - datetime.datetime(2013, 1, 1)).dt.days

data_combine_lstm.head(10)

col_lstm = [i for i in data_combine_lstm.columns if i not in ['date','id']]

print("Old train shape: ", train.shape)
train_new_lstm = data_combine_lstm.loc[~data_combine_lstm.sales.isna()]
print("New train shape: ",train_new_lstm.shape)
print('---')
print("Old test shape: ", test.shape)
test_new_lstm = data_combine_lstm.loc[data_combine_lstm.sales.isna()]
print("New test shape: ",test_new_lstm.shape)

train_new_lstm = (train_new_lstm[col_lstm]).dropna()
print(train_new_lstm.shape) # No NaN values, since we do not use the shift

y_target = train_new_lstm.sales
col = [i for i in data_combine_lstm.columns if i not in ['date','id','sales']]
print(col)

# convert series to supervised learning
def series_to_supervised(data, n_in=60, n_out=1, dropnan=True):
  n_vars = 1 if type(data) is list else data.shape[1]
  df = pd.DataFrame(data)
  cols, names = list(), list()
  # input sequence (t-n, ... t-1)
  for i in range(n_in, 0, -1):
    cols.append(df.shift(i))
    names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]
  # forecast sequence (t, t+1, ... t+n)
  for i in range(0, n_out):
    cols.append(df.shift(-i))
  if i == 0:
    names += [('var%d(t)' % (j+1)) for j in range(n_vars)]
  else:
    names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]
    # put it all together
  agg = pd.concat(cols, axis=1)
  agg.columns = names
  # drop rows with NaN values
  if dropnan:
    agg.dropna(inplace=True)
  return agg

cols_lstm = list(train_new_lstm.columns)
cols_lstm =  cols_lstm[2:3] + cols_lstm[:2] + cols_lstm[3:]
cols_lstm
train_new_lstm = train_new_lstm[cols_lstm]
train_new_lstm.head(3)

values = train_new_lstm.values
# ensure all data is float
values = values.astype('float32')
# normalize features
scaler = MinMaxScaler(feature_range=(0, 1))
scaled = scaler.fit_transform(values)
# frame as supervised learning
reframed = series_to_supervised(scaled, 1, 1)
# drop columns we don't want to predict
reframed.drop(reframed.columns[[9,10,11,12,13,14,15]], axis=1, inplace=True)
print(reframed.head())

reframed.head()

# split into train and test sets
values = reframed.values

train_length = int(train_new_lstm.shape[0]*0.85)
train_temp = values[0:train_length, :]
test_temp = values[train_length:, :]

# split into input and outputs
train_X, train_y = train_temp[:, :-1], train_temp[:, -1]
test_X, test_y = test_temp[:, :-1], test_temp[:, -1]
# reshape input to be 3D [samples, timesteps, features]
train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))
test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))
print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)

# design network
model = Sequential()
model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))
model.add(Dense(1))
model.compile(loss='mae', optimizer='adam')
# fit network
history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), shuffle=False,
                    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, min_delta=0.0001)])
# plot history
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='test')
pyplot.legend()
pyplot.show()

print(test_X.shape)

# make a prediction
yhat = model.predict(test_X)
test_X_new = test_X.reshape((test_X.shape[0], test_X.shape[2]))

#print(yhat.shape)
#print(inv_yhat.shape)
#print(test_X_new[:, 1:].shape)

# invert scaling for forecast
inv_yhat = concatenate((yhat, test_X_new[:, 1:]), axis=1)
inv_yhat = scaler.inverse_transform(inv_yhat)
#print(inv_yhat.shape)
inv_yhat = inv_yhat[:,0]
# invert scaling for actual
test_y = test_y.reshape((len(test_y), 1))
inv_y = concatenate((test_y, test_X_new[:, 1:]), axis=1)
inv_y = scaler.inverse_transform(inv_y)
inv_y = inv_y[:,0]
# calculate RMSE
rmse = sqrt(mean_squared_error(inv_y, inv_yhat))
print('Test RMSE: %.3f' % rmse)

"""Clearly LSTM does not perform as well as XGBoost"""